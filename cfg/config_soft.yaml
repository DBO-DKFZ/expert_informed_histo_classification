defaults:
- metrics:  # https://lightning.ai/docs/torchmetrics/stable/
  - AUROC
  - Accuracy
  - F1Score
- callbacks: # useful link: https://lightning.ai/docs/pytorch/stable/api_references.html#callbacks
  # Note, that LearningRateMonitor('epoch') is added automatically. You can override it, by setting LearningRateMonitor here.
  - EarlyStopping
  - ModelCheckpoint
- _self_

job: test # train, tune, test, convert_hdf5
metric: val_loss
mode: min # whether to max or min the metric
checkpoint: models/HistoSoftLabels.ckpt

task:
  classification_task: multiclass
  num_classes: 3

loss:
  _target_: torch.nn.CrossEntropyLoss
  weight:
    _target_: torch.tensor
    _args_: [[1.5, 2., 1.]]

dataset:
  data_set:
    _target_: hydra.utils.get_class
    path: datasets.HDF5GraphClassification
  data_dir: data/tcga-demo.hdf5
  graph_data: True
  transform_train_gpu:
    _target_: torch_geometric.transforms.Compose
    transforms:
      - _target_: torch_geometric.transforms.NormalizeFeatures
  transform_test_gpu:
    _target_: torch_geometric.transforms.Compose
    transforms:
      - _target_: torch_geometric.transforms.NormalizeFeatures

split:
  mode: single
  stratify: True
  kwargs:
    train_size: 0.8

model:
  _target_: models.GNN
  gnn: None
  pool:
    pool_final:
      in_out:  'x, batch, pos -> x'
      module:
        _target_: models.TransformerAggregation
        d_model: 512
        nhead: 8
        num_layers: 4
        dropout: ${hyper.dropout}
        pos_enc: False
  head:
    _target_: torch.nn.Linear
    in_features: ${model.pool.pool_final.module.d_model}
    out_features: ${task.num_classes}

scheduler:
  _target_: ray.tune.schedulers.ASHAScheduler

lr_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
  T_0: 5

# use temperature scaling (currently only used in test.py)
temp_scaling:
  use: False
  value:
    _target_: torch.tensor
    _args_: [[2.25463799, 1.47872218, 2.1373289]]

# trainer parameters
trainer_params:
  max_epochs: 50
  precision: 16-mixed # "16-mixed" for mixed precision, "32-true" for 32-bit precision, etc. Can also specify "bf16-mixed" or "bf16-true", ...
  sync_batchnorm: True # whether to sync batch norm layers. Only relevant for multi-gpu training
  deterministic: True

# TRAINING
optimizer:
  _target_: torch.optim.AdamW
  lr: 6.597107317077041e-05
  weight_decay: 2.2787823111535095e-05
batch_size: 2
hyper:
  dropout: 0.5
accumulate_grad_batches: 16

# https://github.com/Schinkikami/PyT-ImbalancedDatasetSampler
imbalanced_sampler:
  use: False
  kwargs:
    num_classes: ${task.num_classes}
    sampling_factor: 0.7

resources:
  num_workers: 1  # number of workers per training when tuning (each trial gets this number, so be careful!)
  cpu_worker: 4 # CPUs per worker
  gpu_worker: 1 # GPUs per worker

# any hyperparameter specified here will override the parameter if it was set before
# instantiates like {param: _target_(_args_)} (e.g.: lr: _target_: ray.tune.loguniform _args_: [1e-4, 1e-1] -> {lr: ray.tune.loguniform(1e-4, 1e-1)})
# note: elements passed as a list in _args_ are treated as individual arguments -> use a list of lists to pass a list as an argument
hyperparameter_tuning:
  optimizer:
    lr:
      _target_: ray.tune.loguniform
      _args_: [1e-6, 1e-3]
    weight_decay:
      _target_: ray.tune.loguniform
      _args_: [1e-5, 1e-3]

# refer to https://docs.ray.io/en/latest/tune/api/doc/ray.tune.TuneConfig.html
tune_config:
  search_alg:
    _target_: ray.tune.search.optuna.OptunaSearch
  kwargs:
    metric: ${metric}
    mode: ${mode}
    num_samples: 80

run_config:
  checkpoint_config:
    _target_: ray.air.config.CheckpointConfig
    num_to_keep: 1
    checkpoint_score_attribute: ${metric}
    checkpoint_score_order: ${mode}

